{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dd4df51",
   "metadata": {},
   "source": [
    "<h1>This is a python notebook containing our implmementation of a RAG for code generation, completion, debugging and other code relatied \"natural\" language queries</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "851e1794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (1.0.7)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (4.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: bs4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.21.1)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (14.0.0)\n",
      "Requirement already satisfied: fastapi==0.115.9 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.115.9)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.19.2)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (3.10.16)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (2.0.2)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (4.13.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.53b1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (2.11.3)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (4.0.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (1.32.1)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from chromadb) (0.34.2)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from fastapi==0.115.9->chromadb) (0.45.3)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (8.6.1)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from build>=1.0.3->chromadb) (25.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2025.4.26)\n",
      "Requirement already satisfied: idna in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb) (3.21.0)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
      "Requirement already satisfied: requests in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.39.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.32.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.32.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.32.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.53b1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.53b1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.53b1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Requirement already satisfied: pydantic-core==2.33.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.33.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sentence_transformers) (2.7.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sentence_transformers) (4.51.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sentence_transformers) (1.6.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sentence_transformers) (11.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from bs4) (4.13.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from beautifulsoup4->bs4) (2.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb) (3.5.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.6.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb sentence_transformers pandas bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b685d3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\katar\\miniconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "import time\n",
    "import uuid\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19bce7",
   "metadata": {},
   "source": [
    "Read the Stackoverflow questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6265237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 500 questions\n"
     ]
    }
   ],
   "source": [
    "data = ['python_questions0.csv']\n",
    "MAX_DOCS = 500\n",
    "df = pd.DataFrame()\n",
    "for d in data:\n",
    "    df = pd.concat([df, pd.read_csv(d)], ignore_index=True)\n",
    "\n",
    "    \n",
    "df = df.loc[:min(len(df), MAX_DOCS-1), [\"tags\", \"question_title\", \"question_body\", \"answer\", \"question_score\"]]\n",
    "total_docs = len(df)\n",
    "print(f\"Loaded {total_docs} questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86743d46",
   "metadata": {},
   "source": [
    "Chunk the questions and prepare them to be embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0ec7133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 1412 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "min_code_block = 10\n",
    "\n",
    "for ix, content in df.iterrows():\n",
    "    answer = content.loc['answer']\n",
    "    tags = content.loc[\"tags\"]\n",
    "    score = content.loc[\"question_score\"]\n",
    "\n",
    "    question_chunk = f\"{content.loc['question_title']}\\n{content.loc['question_body']}\".lower()\n",
    "    chunks.append({\"chunk\": question_chunk,\n",
    "                   \"metadata\": {\"tags\": tags,\n",
    "                                \"score\": score,\n",
    "                                \"question\": True,\n",
    "                                \"code\": False,\n",
    "                                \"answer\": answer.lower()\n",
    "                                }})\n",
    "    \n",
    "    answer_chunk = str(answer).lower()\n",
    "    chunks.append({\"chunk\": answer_chunk,\n",
    "                   \"metadata\": {\"tags\": tags,\n",
    "                   \"score\": score,\n",
    "                   \"question\": False,\n",
    "                   \"code\": False}\n",
    "                   })\n",
    "\n",
    "    soup = BeautifulSoup(answer, 'html.parser')\n",
    "    code_blocks = [code.get_text() for code in soup.find_all('code')]\n",
    "    for block in code_blocks:\n",
    "        if len(block) > min_code_block and '\\n' in block.strip():\n",
    "            chunks.append({\"chunk\": block.lower(),\n",
    "                           \"metadata\": {\"tags\": tags,\n",
    "                                        \"score\": score,\n",
    "                                        \"question\": False,\n",
    "                                        \"code\": True}})\n",
    "\n",
    "chunks = pd.DataFrame(chunks)\n",
    "total_chunks = len(chunks)\n",
    "print(f\"Prepared {total_chunks} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e81d3e",
   "metadata": {},
   "source": [
    "Initiate the embedder model and the vector database to store embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "337b3cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Chroma client\n",
    "client = chromadb.PersistentClient(path=\"./test_db\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"stackoverflow_demo\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4129acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd88a21",
   "metadata": {},
   "source": [
    "Embed the chunks and save them into the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeade83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_progress(current, total, start_time, operation=\"Processing\"):\n",
    "    elapsed = time.time() - start_time\n",
    "    percent = current / total\n",
    "    eta = (elapsed / current) * (total - current) if current > 0 else 0\n",
    "    print(\n",
    "        f\"\\r{operation}: {current}/{total} ({percent:.1%}) | \"\n",
    "        f\"Elapsed: {elapsed:.1f}s | ETA: {eta:.1f}s\",\n",
    "        end=\"\", flush=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97e3462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: 1412/1412 (100.0%) | Elapsed: 78.5s | ETA: 0.0s\n",
      "\n",
      "Successfully added 1412 documents\n",
      "Total documents in collection: 2412\n",
      "Total time: 78.52 seconds\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 200\n",
    "total_added = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for batch_num in range(0, total_chunks, BATCH_SIZE):\n",
    "    batch = chunks.iloc[batch_num:batch_num + BATCH_SIZE]\n",
    "    \n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "    embeddings = []\n",
    "    \n",
    "    for ix, row in batch.iterrows():\n",
    "        chunk = row[\"chunk\"]\n",
    "        metadata = row[\"metadata\"]\n",
    "        documents.append(chunk)\n",
    "        metadatas.append(metadata)\n",
    "        ids.append(str(uuid.uuid4()))\n",
    "        embeddings.append(model.encode(chunk).tolist())\n",
    "    \n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=metadatas,\n",
    "        ids=ids,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    total_added += len(documents)\n",
    "\n",
    "    print_progress(min(batch_num + BATCH_SIZE, total_chunks), total_chunks, start_time)\n",
    "\n",
    "\n",
    "print(f\"\\n\\nSuccessfully added {total_added} documents\")\n",
    "print(f\"Total documents in collection: {collection.count()}\")\n",
    "print(f\"Total time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dbff415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 2412\n",
      "\n",
      "Document 1:\n",
      "ID: 20eb6424-2233-47aa-9fe4-6a17e81056e4\n",
      "Content: deleting dataframe row in pandas based on column value\n",
      "<p>i have the following dataframe:</p>\n",
      "\n",
      "<pre><code>             daysago  line_race rating        rw    wrating\n",
      " line_date                        ...\n",
      "Metadata: {'answer': \"<p>the given answer is correct nontheless as someone above said you can use <code>df.query('line_race != 0')</code> which depending on your problem is much faster. highly recommend.</p>\", 'score': 256, 'tags': 'python|pandas', 'code': False, 'question': True}\n",
      "\n",
      "Document 2:\n",
      "ID: 2f563cda-e2fc-4134-b81f-18216853484e\n",
      "Content: <p>the given answer is correct nontheless as someone above said you can use <code>df.query('line_race != 0')</code> which depending on your problem is much faster. highly recommend.</p>...\n",
      "Metadata: {'code': False, 'score': 256, 'tags': 'python|pandas', 'question': False}\n",
      "\n",
      "Document 3:\n",
      "ID: 982cf52e-d6ee-4204-8c32-ed0525a41cb8\n",
      "Content: deleting dataframe row in pandas based on column value\n",
      "<p>i have the following dataframe:</p>\n",
      "\n",
      "<pre><code>             daysago  line_race rating        rw    wrating\n",
      " line_date                        ...\n",
      "Metadata: {'tags': 'python|pandas', 'answer': \"<p>the best way to do this is with boolean masking:</p>\\n\\n<pre><code>in [56]: df\\nout[56]:\\n     line_date  daysago  line_race  rating    raw  wrating\\n0   2007-03-31       62         11      56  1.000   56.000\\n1   2007-03-10       83         11      67  1.000   67.000\\n2   2007-02-10      111          9      66  1.000   66.000\\n3   2007-01-13      139         10      83  0.881   73.096\\n4   2006-12-23      160         10      88  0.793   69.787\\n5   2006-11-09      204          9      52  0.637   33.106\\n6   2006-10-22      222          8      66  0.582   38.408\\n7   2006-09-29      245          9      70  0.519   36.318\\n8   2006-09-16      258         11      68  0.486   33.063\\n9   2006-08-30      275          8      72  0.447   32.160\\n10  2006-02-11      475          5      65  0.165   10.698\\n11  2006-01-13      504          0      70  0.142    9.969\\n12  2006-01-02      515          0      64  0.135    8.627\\n13  2005-12-06      542          0      70  0.118    8.246\\n14  2005-11-29      549          0      70  0.114    7.963\\n15  2005-11-22      556          0      -1  0.110   -0.110\\n16  2005-11-01      577          0      -1  0.099   -0.099\\n17  2005-10-20      589          0      -1  0.093   -0.093\\n18  2005-09-27      612          0      -1  0.083   -0.083\\n19  2005-09-07      632          0      -1  0.075   -0.075\\n20  2005-06-12      719          0      69  0.049    3.360\\n21  2005-05-29      733          0      -1  0.045   -0.045\\n22  2005-05-02      760          0      -1  0.040   -0.040\\n23  2005-04-02      790          0      -1  0.034   -0.034\\n24  2005-03-13      810          0      -1  0.031   -0.031\\n25  2004-11-09      934          0      -1  0.017   -0.017\\n\\nin [57]: df[df.line_race != 0]\\nout[57]:\\n     line_date  daysago  line_race  rating    raw  wrating\\n0   2007-03-31       62         11      56  1.000   56.000\\n1   2007-03-10       83         11      67  1.000   67.000\\n2   2007-02-10      111          9      66  1.000   66.000\\n3   2007-01-13      139         10      83  0.881   73.096\\n4   2006-12-23      160         10      88  0.793   69.787\\n5   2006-11-09      204          9      52  0.637   33.106\\n6   2006-10-22      222          8      66  0.582   38.408\\n7   2006-09-29      245          9      70  0.519   36.318\\n8   2006-09-16      258         11      68  0.486   33.063\\n9   2006-08-30      275          8      72  0.447   32.160\\n10  2006-02-11      475          5      65  0.165   10.698\\n</code></pre>\\n\\n<p><strong>update:</strong> now that pandas 0.13 is out, another way to do this is <code>df.query('line_race != 0')</code>.</p>\", 'score': 256, 'code': False, 'question': True}\n"
     ]
    }
   ],
   "source": [
    "results = collection.get()\n",
    "print(f\"Total documents: {len(results['ids'])}\")\n",
    "\n",
    "# Inspect first few items\n",
    "for i in range(min(3, len(results['ids']))):\n",
    "    print(f\"\\nDocument {i+1}:\")\n",
    "    print(f\"ID: {results['ids'][i]}\")\n",
    "    print(f\"Content: {results['documents'][i][:200]}...\")  # First 200 chars\n",
    "    print(f\"Metadata: {results['metadatas'][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5f7ae4",
   "metadata": {},
   "source": [
    "Check the retrieval process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "436fbc8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 3 similar questions:\n",
      "\n",
      "Result 1:\n",
      "Score: 0.58\n",
      "Content: import json\n",
      "import pandas as pd\n",
      "from pandas.io.json import json_normalize\n",
      "import csv\n",
      "\n",
      "source_file = '11april1.txt'\n",
      "result_file = 'output.csv'\n",
      "\n",
      "\n",
      "with open(source_file) as source:\n",
      "    with open(result_f...\n",
      "Tags: python|json|pandas\n",
      "\n",
      "Result 2:\n",
      "Score: 0.56\n",
      "Content: import json\n",
      "\n",
      "foo = {none: 7, 'bar': 8}\n",
      "# {'bar': 8, none: 7}\n",
      "\n",
      "foo_json = json.dumps(foo)\n",
      "# '{\"bar\": 8, \"null\": 7}'\n",
      "\n",
      "foo_prime = json.loads(foo_json)\n",
      "# {'null': 7, 'bar': 8}\n",
      "\n",
      "foo_sorted = json.dumps(fo...\n",
      "Tags: python|json\n",
      "\n",
      "Result 3:\n",
      "Score: 0.56\n",
      "Content: import json\n",
      "\n",
      "foo = {none: 7, 'bar': 8}\n",
      "# {'bar': 8, none: 7}\n",
      "\n",
      "foo_json = json.dumps(foo)\n",
      "# '{\"bar\": 8, \"null\": 7}'\n",
      "\n",
      "foo_prime = json.loads(foo_json)\n",
      "# {'null': 7, 'bar': 8}\n",
      "\n",
      "foo_sorted = json.dumps(fo...\n",
      "Tags: python|json\n"
     ]
    }
   ],
   "source": [
    "# Search for similar questions\n",
    "query_text = \"how to parse json in python\"\n",
    "query_embedding = model.encode(query_text.lower()).tolist()\n",
    "\n",
    "results = collection.query(\n",
    "    query_embeddings=[query_embedding],\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "print(\"\\nTop 3 similar questions:\")\n",
    "for i, (doc, meta) in enumerate(zip(results['documents'][0], results['metadatas'][0])):\n",
    "    print(f\"\\nResult {i+1}:\")\n",
    "    print(f\"Score: {1 - results['distances'][0][i]:.2f}\")\n",
    "    print(f\"Content: {doc[:200]}...\")\n",
    "    print(f\"Tags: {meta['tags']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08935353",
   "metadata": {},
   "source": [
    "### Adding a reranker\n",
    "\n",
    "Compare the results received without reranker (above cell code) with the results gotten with a reranker (below).\n",
    "\n",
    "First implementation of reranker is using two different cross encoder models MiniLM-L-6-v2 and bge-reranker-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f07994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 reranked results for MiniLM-L-6-v2:\n",
      "------------------------------------------------------------\n",
      "Result 1:\n",
      "Score: 4.49\n",
      "Content: efficient way to parsing from tweets json formated files\n",
      "<p>i'm parsing from tweets data which is json format and compressed with gzip.</p>\n",
      "\n",
      "<p>here's my code:</p>\n",
      "\n",
      "<pre><code>###preprocessing\n",
      "##impor...\n",
      "Tags: json|python-3.x|encoding\n",
      "Answer: <p>here is your updated code after the <code>defualt --&gt; default</code> correction:</p>\n",
      "\n",
      "<pre><code>import json\n",
      "\n",
      "class contact:\n",
      "  def __init__(self, first, last):\n",
      "    self.first = first\n",
      "    self.last = last\n",
      "\n",
      "  @property\n",
      "  def full_name(self):\n",
      "    return (\"{} {}\".format(self.first, self.last))\n",
      "\n",
      "class contactencoder(json.jsonencoder):\n",
      "  def default(self, obj):\n",
      "    if isinstance(obj, contact):\n",
      "      return  {\"is_contact\": 't'\n",
      "              ,\"first\": obj.first\n",
      "              ,\"last\": obj.last\n",
      "              ,\"full_name\": obj.full_name}\n",
      "    return super().default(obj)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  c = contact(\"jay\", \"loophole\")\n",
      "  print(json.dumps(c.__dict__))\n",
      "  print(json.dumps(c, cls=contactencoder))\n",
      "</code></pre>\n",
      "\n",
      "<p>you can check it out live on <a href=\"http://ideone.com/ue15rk\" rel=\"nofollow\">this page</a>.</p>\n",
      "Code Block: No\n",
      "Top 1 reranked results for bge-reranker-base:\n",
      "------------------------------------------------------------\n",
      "Result 1:\n",
      "Score: 0.70\n",
      "Content: <p>using <a href=\"http://docs.python-requests.org/en/master/\" rel=\"nofollow noreferrer\">python-requests</a></p>\n",
      "\n",
      "<p>this code merges all the <code>json</code> received from the urls into one <code>fin...\n",
      "Tags: json|python-3.x|encoding\n",
      "Answer: <p>here is your updated code after the <code>defualt --&gt; default</code> correction:</p>\n",
      "\n",
      "<pre><code>import json\n",
      "\n",
      "class contact:\n",
      "  def __init__(self, first, last):\n",
      "    self.first = first\n",
      "    self.last = last\n",
      "\n",
      "  @property\n",
      "  def full_name(self):\n",
      "    return (\"{} {}\".format(self.first, self.last))\n",
      "\n",
      "class contactencoder(json.jsonencoder):\n",
      "  def default(self, obj):\n",
      "    if isinstance(obj, contact):\n",
      "      return  {\"is_contact\": 't'\n",
      "              ,\"first\": obj.first\n",
      "              ,\"last\": obj.last\n",
      "              ,\"full_name\": obj.full_name}\n",
      "    return super().default(obj)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "  c = contact(\"jay\", \"loophole\")\n",
      "  print(json.dumps(c.__dict__))\n",
      "  print(json.dumps(c, cls=contactencoder))\n",
      "</code></pre>\n",
      "\n",
      "<p>you can check it out live on <a href=\"http://ideone.com/ue15rk\" rel=\"nofollow\">this page</a>.</p>\n",
      "Code Block: No\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Search for similar questions and rerank them, use this example for testing\n",
    "query_text = \"how to parse json in python\"\n",
    "query_embedding = model.encode(query_text.lower()).tolist()\n",
    "\n",
    "# use more (eg. 10-20) results for reranking\n",
    "initial_results = collection.query(query_embeddings=[query_embedding], n_results=20) \n",
    "\n",
    "docs = initial_results['documents'][0]\n",
    "distances = initial_results['distances'][0]\n",
    "\n",
    "# prepare pairs for CrossEncoder\n",
    "pairs = [(query_text, doc) for doc in docs]\n",
    "\n",
    "# CrossEncoder MiniLM-L-6-v2 \n",
    "reranker1 = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "scores1 = reranker1.predict(pairs)\n",
    "\n",
    "# CrossEncoder bge-reranker-base\n",
    "reranker2 = CrossEncoder('BAAI/bge-reranker-base')\n",
    "scores2 = reranker2.predict(pairs)\n",
    "\n",
    "# Sort documents based on reranked scores\n",
    "def get_highest_scores(scores):\n",
    "    list_of_documents_scores = list(zip(docs, scores))\n",
    "    return sorted(list_of_documents_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "scores_ranked_1 = get_highest_scores(scores1)\n",
    "scores_ranked_2 = get_highest_scores(scores2)\n",
    "\n",
    "# Check reranked results to our query\n",
    "number_of_results = 1\n",
    "print(f\"Top {number_of_results} reranked results for MiniLM-L-6-v2:\")\n",
    "for i, (doc, score) in enumerate(scores_ranked_1[:number_of_results]):\n",
    "    print('-' * 60)\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(f\"Content: {doc[:200]}...\")\n",
    "    print(f\"Tags: {meta['tags']}\")\n",
    "    print(f\"Answer: {meta.get('answer', 'N/A')}\")\n",
    "    print(f\"Code Block: {'Yes' if meta.get('code', False) else 'No'}\")\n",
    "\n",
    "print(f\"Top {number_of_results} reranked results for bge-reranker-base:\")\n",
    "for i, (doc, score) in enumerate(scores_ranked_2[:number_of_results]):\n",
    "    print('-' * 60)\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(f\"Content: {doc[:200]}...\")\n",
    "    print(f\"Tags: {meta['tags']}\")\n",
    "    print(f\"Answer: {meta.get('answer', 'N/A')}\")\n",
    "    print(f\"Code Block: {'Yes' if meta.get('code', False) else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec1d5b",
   "metadata": {},
   "source": [
    "Implement another reranker: the ColBERT (Contextualized Late Interaction over BERT) using HuggingFace (not a full FAISS retrieval)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b917e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColBert Ranked Results:\n",
      "------------------------------------------------------------\n",
      "Result 1:\n",
      "Score: 0.76\n",
      "Content: efficient way to parsing from tweets json formated files\n",
      "<p>i'm parsing from tweets data which is json format and compressed with gzip.</p>\n",
      "\n",
      "<p>here's my code:</p>\n",
      "\n",
      "<pre><code>###preprocessing\n",
      "##impor...\n",
      "Code Block: No\n",
      "------------------------------------------------------------\n",
      "Result 2:\n",
      "Score: 0.76\n",
      "Content: efficient way to parsing from tweets json formated files\n",
      "<p>i'm parsing from tweets data which is json format and compressed with gzip.</p>\n",
      "\n",
      "<p>here's my code:</p>\n",
      "\n",
      "<pre><code>###preprocessing\n",
      "##impor...\n",
      "Code Block: No\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\").to(device)\n",
    "colbert_model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_colbert_encoding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    outputs = colbert_model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :]  #use the first token's embedding\n",
    "    return embeddings.cpu()\n",
    "\n",
    "def colbert_score(query, docs):\n",
    "    query_embedding = get_colbert_encoding(query)\n",
    "    doc_embeddings = torch.cat([get_colbert_encoding(doc) for doc in docs], dim=0) \n",
    "\n",
    "    query_embedding = query_embedding.expand(doc_embeddings.size(0), -1)\n",
    "\n",
    "    similarity_scores = F.cosine_similarity(query_embedding, doc_embeddings, dim=1)\n",
    "\n",
    "    return similarity_scores.tolist()\n",
    "\n",
    "# Compare ColBERT scores with cross-encoder scores on this query text\n",
    "query_text = \"how to parse json in python\"\n",
    "\n",
    "# Already done in previous two cells:\n",
    "'''\n",
    "initial_results = collection.query(query_embeddings=[model.encode(query_text.lower()).tolist()], n_results=20)\n",
    "docs = initial_results['documents'][0]\n",
    "distances = initial_results['distances'][0]\n",
    "'''\n",
    "\n",
    "colbert_scores = colbert_score(query_text, docs)\n",
    "colbert_ranked = sorted(zip(docs, colbert_scores), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "number_of_results = 2\n",
    "print('ColBert Ranked Results:')\n",
    "for i, (doc, score) in enumerate(colbert_ranked[:number_of_results]):\n",
    "    print('-' * 60)\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(f\"Score: {score:.2f}\")\n",
    "    print(f\"Content: {doc[:200]}...\")\n",
    "    print(f\"Code Block: {'Yes' if meta.get('code', False) else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0805a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: requests in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: accelerate in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: requests in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers --index-url https://download.pytorch.org/whl/cpu\n",
    "! pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a0d77af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages (from requests->transformers) (3.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.1.3; however, version 25.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\38641\\documents\\faks\\5.letnik\\2.semester\\nlp\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b494b8d1",
   "metadata": {},
   "source": [
    "Test the whole RAG pipeline on a small LLM that runs on CPU. Compare results to plain LLM, to see if retrieval even helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2d0044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\38641\\Documents\\faks\\5.letnik\\2.semester\\NLP\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# for testing only\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mRAG\u001b[39;00m:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedder, collection, retrieve_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, gpu_based\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, history_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\38641\\Documents\\faks\\5.letnik\\2.semester\\NLP\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1955\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1953\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1954\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1955\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1956\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32mc:\\Users\\38641\\Documents\\faks\\5.letnik\\2.semester\\NLP\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1967\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1967\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1970\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1972\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\38641\\Documents\\faks\\5.letnik\\2.semester\\NLP\\ul-fri-nlp-course-project-2024-2025-1-6-3-musketeers\\venv\\lib\\site-packages\\transformers\\models\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     albert,\n\u001b[0;32m     17\u001b[0m     align,\n\u001b[0;32m     18\u001b[0m     altclip,\n\u001b[0;32m     19\u001b[0m     aria,\n\u001b[0;32m     20\u001b[0m     audio_spectrogram_transformer,\n\u001b[0;32m     21\u001b[0m     auto,\n\u001b[0;32m     22\u001b[0m     autoformer,\n\u001b[0;32m     23\u001b[0m     aya_vision,\n\u001b[0;32m     24\u001b[0m     bamba,\n\u001b[0;32m     25\u001b[0m     bark,\n\u001b[0;32m     26\u001b[0m     bart,\n\u001b[0;32m     27\u001b[0m     barthez,\n\u001b[0;32m     28\u001b[0m     bartpho,\n\u001b[0;32m     29\u001b[0m     beit,\n\u001b[0;32m     30\u001b[0m     bert,\n\u001b[0;32m     31\u001b[0m     bert_generation,\n\u001b[0;32m     32\u001b[0m     bert_japanese,\n\u001b[0;32m     33\u001b[0m     bertweet,\n\u001b[0;32m     34\u001b[0m     big_bird,\n\u001b[0;32m     35\u001b[0m     bigbird_pegasus,\n\u001b[0;32m     36\u001b[0m     biogpt,\n\u001b[0;32m     37\u001b[0m     bit,\n\u001b[0;32m     38\u001b[0m     blenderbot,\n\u001b[0;32m     39\u001b[0m     blenderbot_small,\n\u001b[0;32m     40\u001b[0m     blip,\n\u001b[0;32m     41\u001b[0m     blip_2,\n\u001b[0;32m     42\u001b[0m     bloom,\n\u001b[0;32m     43\u001b[0m     bridgetower,\n\u001b[0;32m     44\u001b[0m     bros,\n\u001b[0;32m     45\u001b[0m     byt5,\n\u001b[0;32m     46\u001b[0m     camembert,\n\u001b[0;32m     47\u001b[0m     canine,\n\u001b[0;32m     48\u001b[0m     chameleon,\n\u001b[0;32m     49\u001b[0m     chinese_clip,\n\u001b[0;32m     50\u001b[0m     clap,\n\u001b[0;32m     51\u001b[0m     clip,\n\u001b[0;32m     52\u001b[0m     clipseg,\n\u001b[0;32m     53\u001b[0m     clvp,\n\u001b[0;32m     54\u001b[0m     code_llama,\n\u001b[0;32m     55\u001b[0m     codegen,\n\u001b[0;32m     56\u001b[0m     cohere,\n\u001b[0;32m     57\u001b[0m     cohere2,\n\u001b[0;32m     58\u001b[0m     colpali,\n\u001b[0;32m     59\u001b[0m     conditional_detr,\n\u001b[0;32m     60\u001b[0m     convbert,\n\u001b[0;32m     61\u001b[0m     convnext,\n\u001b[0;32m     62\u001b[0m     convnextv2,\n\u001b[0;32m     63\u001b[0m     cpm,\n\u001b[0;32m     64\u001b[0m     cpmant,\n\u001b[0;32m     65\u001b[0m     ctrl,\n\u001b[0;32m     66\u001b[0m     cvt,\n\u001b[0;32m     67\u001b[0m     dab_detr,\n\u001b[0;32m     68\u001b[0m     dac,\n\u001b[0;32m     69\u001b[0m     data2vec,\n\u001b[0;32m     70\u001b[0m     dbrx,\n\u001b[0;32m     71\u001b[0m     deberta,\n\u001b[0;32m     72\u001b[0m     deberta_v2,\n\u001b[0;32m     73\u001b[0m     decision_transformer,\n\u001b[0;32m     74\u001b[0m     deepseek_v3,\n\u001b[0;32m     75\u001b[0m     deformable_detr,\n\u001b[0;32m     76\u001b[0m     deit,\n\u001b[0;32m     77\u001b[0m     deprecated,\n\u001b[0;32m     78\u001b[0m     depth_anything,\n\u001b[0;32m     79\u001b[0m     depth_pro,\n\u001b[0;32m     80\u001b[0m     detr,\n\u001b[0;32m     81\u001b[0m     dialogpt,\n\u001b[0;32m     82\u001b[0m     diffllama,\n\u001b[0;32m     83\u001b[0m     dinat,\n\u001b[0;32m     84\u001b[0m     dinov2,\n\u001b[0;32m     85\u001b[0m     dinov2_with_registers,\n\u001b[0;32m     86\u001b[0m     distilbert,\n\u001b[0;32m     87\u001b[0m     dit,\n\u001b[0;32m     88\u001b[0m     donut,\n\u001b[0;32m     89\u001b[0m     dpr,\n\u001b[0;32m     90\u001b[0m     dpt,\n\u001b[0;32m     91\u001b[0m     efficientnet,\n\u001b[0;32m     92\u001b[0m     electra,\n\u001b[0;32m     93\u001b[0m     emu3,\n\u001b[0;32m     94\u001b[0m     encodec,\n\u001b[0;32m     95\u001b[0m     encoder_decoder,\n\u001b[0;32m     96\u001b[0m     ernie,\n\u001b[0;32m     97\u001b[0m     esm,\n\u001b[0;32m     98\u001b[0m     falcon,\n\u001b[0;32m     99\u001b[0m     falcon_mamba,\n\u001b[0;32m    100\u001b[0m     fastspeech2_conformer,\n\u001b[0;32m    101\u001b[0m     flaubert,\n\u001b[0;32m    102\u001b[0m     flava,\n\u001b[0;32m    103\u001b[0m     fnet,\n\u001b[0;32m    104\u001b[0m     focalnet,\n\u001b[0;32m    105\u001b[0m     fsmt,\n\u001b[0;32m    106\u001b[0m     funnel,\n\u001b[0;32m    107\u001b[0m     fuyu,\n\u001b[0;32m    108\u001b[0m     gemma,\n\u001b[0;32m    109\u001b[0m     gemma2,\n\u001b[0;32m    110\u001b[0m     gemma3,\n\u001b[0;32m    111\u001b[0m     git,\n\u001b[0;32m    112\u001b[0m     glm,\n\u001b[0;32m    113\u001b[0m     glm4,\n\u001b[0;32m    114\u001b[0m     glpn,\n\u001b[0;32m    115\u001b[0m     got_ocr2,\n\u001b[0;32m    116\u001b[0m     gpt2,\n\u001b[0;32m    117\u001b[0m     gpt_bigcode,\n\u001b[0;32m    118\u001b[0m     gpt_neo,\n\u001b[0;32m    119\u001b[0m     gpt_neox,\n\u001b[0;32m    120\u001b[0m     gpt_neox_japanese,\n\u001b[0;32m    121\u001b[0m     gpt_sw3,\n\u001b[0;32m    122\u001b[0m     gptj,\n\u001b[0;32m    123\u001b[0m     granite,\n\u001b[0;32m    124\u001b[0m     granitemoe,\n\u001b[0;32m    125\u001b[0m     granitemoeshared,\n\u001b[0;32m    126\u001b[0m     grounding_dino,\n\u001b[0;32m    127\u001b[0m     groupvit,\n\u001b[0;32m    128\u001b[0m     helium,\n\u001b[0;32m    129\u001b[0m     herbert,\n\u001b[0;32m    130\u001b[0m     hiera,\n\u001b[0;32m    131\u001b[0m     hubert,\n\u001b[0;32m    132\u001b[0m     ibert,\n\u001b[0;32m    133\u001b[0m     idefics,\n\u001b[0;32m    134\u001b[0m     idefics2,\n\u001b[0;32m    135\u001b[0m     idefics3,\n\u001b[0;32m    136\u001b[0m     ijepa,\n\u001b[0;32m    137\u001b[0m     imagegpt,\n\u001b[0;32m    138\u001b[0m     informer,\n\u001b[0;32m    139\u001b[0m     instructblip,\n\u001b[0;32m    140\u001b[0m     instructblipvideo,\n\u001b[0;32m    141\u001b[0m     jamba,\n\u001b[0;32m    142\u001b[0m     jetmoe,\n\u001b[0;32m    143\u001b[0m     kosmos2,\n\u001b[0;32m    144\u001b[0m     layoutlm,\n\u001b[0;32m    145\u001b[0m     layoutlmv2,\n\u001b[0;32m    146\u001b[0m     layoutlmv3,\n\u001b[0;32m    147\u001b[0m     layoutxlm,\n\u001b[0;32m    148\u001b[0m     led,\n\u001b[0;32m    149\u001b[0m     levit,\n\u001b[0;32m    150\u001b[0m     lilt,\n\u001b[0;32m    151\u001b[0m     llama,\n\u001b[0;32m    152\u001b[0m     llama4,\n\u001b[0;32m    153\u001b[0m     llava,\n\u001b[0;32m    154\u001b[0m     llava_next,\n\u001b[0;32m    155\u001b[0m     llava_next_video,\n\u001b[0;32m    156\u001b[0m     llava_onevision,\n\u001b[0;32m    157\u001b[0m     longformer,\n\u001b[0;32m    158\u001b[0m     longt5,\n\u001b[0;32m    159\u001b[0m     luke,\n\u001b[0;32m    160\u001b[0m     lxmert,\n\u001b[0;32m    161\u001b[0m     m2m_100,\n\u001b[0;32m    162\u001b[0m     mamba,\n\u001b[0;32m    163\u001b[0m     mamba2,\n\u001b[0;32m    164\u001b[0m     marian,\n\u001b[0;32m    165\u001b[0m     markuplm,\n\u001b[0;32m    166\u001b[0m     mask2former,\n\u001b[0;32m    167\u001b[0m     maskformer,\n\u001b[0;32m    168\u001b[0m     mbart,\n\u001b[0;32m    169\u001b[0m     mbart50,\n\u001b[0;32m    170\u001b[0m     megatron_bert,\n\u001b[0;32m    171\u001b[0m     megatron_gpt2,\n\u001b[0;32m    172\u001b[0m     mgp_str,\n\u001b[0;32m    173\u001b[0m     mimi,\n\u001b[0;32m    174\u001b[0m     mistral,\n\u001b[0;32m    175\u001b[0m     mistral3,\n\u001b[0;32m    176\u001b[0m     mixtral,\n\u001b[0;32m    177\u001b[0m     mllama,\n\u001b[0;32m    178\u001b[0m     mluke,\n\u001b[0;32m    179\u001b[0m     mobilebert,\n\u001b[0;32m    180\u001b[0m     mobilenet_v1,\n\u001b[0;32m    181\u001b[0m     mobilenet_v2,\n\u001b[0;32m    182\u001b[0m     mobilevit,\n\u001b[0;32m    183\u001b[0m     mobilevitv2,\n\u001b[0;32m    184\u001b[0m     modernbert,\n\u001b[0;32m    185\u001b[0m     moonshine,\n\u001b[0;32m    186\u001b[0m     moshi,\n\u001b[0;32m    187\u001b[0m     mpnet,\n\u001b[0;32m    188\u001b[0m     mpt,\n\u001b[0;32m    189\u001b[0m     mra,\n\u001b[0;32m    190\u001b[0m     mt5,\n\u001b[0;32m    191\u001b[0m     musicgen,\n\u001b[0;32m    192\u001b[0m     musicgen_melody,\n\u001b[0;32m    193\u001b[0m     mvp,\n\u001b[0;32m    194\u001b[0m     myt5,\n\u001b[0;32m    195\u001b[0m     nemotron,\n\u001b[0;32m    196\u001b[0m     nllb,\n\u001b[0;32m    197\u001b[0m     nllb_moe,\n\u001b[0;32m    198\u001b[0m     nougat,\n\u001b[0;32m    199\u001b[0m     nystromformer,\n\u001b[0;32m    200\u001b[0m     olmo,\n\u001b[0;32m    201\u001b[0m     olmo2,\n\u001b[0;32m    202\u001b[0m     olmoe,\n\u001b[0;32m    203\u001b[0m     omdet_turbo,\n\u001b[0;32m    204\u001b[0m     oneformer,\n\u001b[0;32m    205\u001b[0m     openai,\n\u001b[0;32m    206\u001b[0m     opt,\n\u001b[0;32m    207\u001b[0m     owlv2,\n\u001b[0;32m    208\u001b[0m     owlvit,\n\u001b[0;32m    209\u001b[0m     paligemma,\n\u001b[0;32m    210\u001b[0m     patchtsmixer,\n\u001b[0;32m    211\u001b[0m     patchtst,\n\u001b[0;32m    212\u001b[0m     pegasus,\n\u001b[0;32m    213\u001b[0m     pegasus_x,\n\u001b[0;32m    214\u001b[0m     perceiver,\n\u001b[0;32m    215\u001b[0m     persimmon,\n\u001b[0;32m    216\u001b[0m     phi,\n\u001b[0;32m    217\u001b[0m     phi3,\n\u001b[0;32m    218\u001b[0m     phi4_multimodal,\n\u001b[0;32m    219\u001b[0m     phimoe,\n\u001b[0;32m    220\u001b[0m     phobert,\n\u001b[0;32m    221\u001b[0m     pix2struct,\n\u001b[0;32m    222\u001b[0m     pixtral,\n\u001b[0;32m    223\u001b[0m     plbart,\n\u001b[0;32m    224\u001b[0m     poolformer,\n\u001b[0;32m    225\u001b[0m     pop2piano,\n\u001b[0;32m    226\u001b[0m     prompt_depth_anything,\n\u001b[0;32m    227\u001b[0m     prophetnet,\n\u001b[0;32m    228\u001b[0m     pvt,\n\u001b[0;32m    229\u001b[0m     pvt_v2,\n\u001b[0;32m    230\u001b[0m     qwen2,\n\u001b[0;32m    231\u001b[0m     qwen2_5_vl,\n\u001b[0;32m    232\u001b[0m     qwen2_audio,\n\u001b[0;32m    233\u001b[0m     qwen2_moe,\n\u001b[0;32m    234\u001b[0m     qwen2_vl,\n\u001b[0;32m    235\u001b[0m     qwen3,\n\u001b[0;32m    236\u001b[0m     qwen3_moe,\n\u001b[0;32m    237\u001b[0m     rag,\n\u001b[0;32m    238\u001b[0m     recurrent_gemma,\n\u001b[0;32m    239\u001b[0m     reformer,\n\u001b[0;32m    240\u001b[0m     regnet,\n\u001b[0;32m    241\u001b[0m     rembert,\n\u001b[0;32m    242\u001b[0m     resnet,\n\u001b[0;32m    243\u001b[0m     roberta,\n\u001b[0;32m    244\u001b[0m     roberta_prelayernorm,\n\u001b[0;32m    245\u001b[0m     roc_bert,\n\u001b[0;32m    246\u001b[0m     roformer,\n\u001b[0;32m    247\u001b[0m     rt_detr,\n\u001b[0;32m    248\u001b[0m     rt_detr_v2,\n\u001b[0;32m    249\u001b[0m     rwkv,\n\u001b[0;32m    250\u001b[0m     sam,\n\u001b[0;32m    251\u001b[0m     seamless_m4t,\n\u001b[0;32m    252\u001b[0m     seamless_m4t_v2,\n\u001b[0;32m    253\u001b[0m     segformer,\n\u001b[0;32m    254\u001b[0m     seggpt,\n\u001b[0;32m    255\u001b[0m     sew,\n\u001b[0;32m    256\u001b[0m     sew_d,\n\u001b[0;32m    257\u001b[0m     shieldgemma2,\n\u001b[0;32m    258\u001b[0m     siglip,\n\u001b[0;32m    259\u001b[0m     siglip2,\n\u001b[0;32m    260\u001b[0m     smolvlm,\n\u001b[0;32m    261\u001b[0m     speech_encoder_decoder,\n\u001b[0;32m    262\u001b[0m     speech_to_text,\n\u001b[0;32m    263\u001b[0m     speecht5,\n\u001b[0;32m    264\u001b[0m     splinter,\n\u001b[0;32m    265\u001b[0m     squeezebert,\n\u001b[0;32m    266\u001b[0m     stablelm,\n\u001b[0;32m    267\u001b[0m     starcoder2,\n\u001b[0;32m    268\u001b[0m     superglue,\n\u001b[0;32m    269\u001b[0m     superpoint,\n\u001b[0;32m    270\u001b[0m     swiftformer,\n\u001b[0;32m    271\u001b[0m     swin,\n\u001b[0;32m    272\u001b[0m     swin2sr,\n\u001b[0;32m    273\u001b[0m     swinv2,\n\u001b[0;32m    274\u001b[0m     switch_transformers,\n\u001b[0;32m    275\u001b[0m     t5,\n\u001b[0;32m    276\u001b[0m     table_transformer,\n\u001b[0;32m    277\u001b[0m     tapas,\n\u001b[0;32m    278\u001b[0m     textnet,\n\u001b[0;32m    279\u001b[0m     time_series_transformer,\n\u001b[0;32m    280\u001b[0m     timesformer,\n\u001b[0;32m    281\u001b[0m     timm_backbone,\n\u001b[0;32m    282\u001b[0m     timm_wrapper,\n\u001b[0;32m    283\u001b[0m     trocr,\n\u001b[0;32m    284\u001b[0m     tvp,\n\u001b[0;32m    285\u001b[0m     udop,\n\u001b[0;32m    286\u001b[0m     umt5,\n\u001b[0;32m    287\u001b[0m     unispeech,\n\u001b[0;32m    288\u001b[0m     unispeech_sat,\n\u001b[0;32m    289\u001b[0m     univnet,\n\u001b[0;32m    290\u001b[0m     upernet,\n\u001b[0;32m    291\u001b[0m     video_llava,\n\u001b[0;32m    292\u001b[0m     videomae,\n\u001b[0;32m    293\u001b[0m     vilt,\n\u001b[0;32m    294\u001b[0m     vipllava,\n\u001b[0;32m    295\u001b[0m     vision_encoder_decoder,\n\u001b[0;32m    296\u001b[0m     vision_text_dual_encoder,\n\u001b[0;32m    297\u001b[0m     visual_bert,\n\u001b[0;32m    298\u001b[0m     vit,\n\u001b[0;32m    299\u001b[0m     vit_mae,\n\u001b[0;32m    300\u001b[0m     vit_msn,\n\u001b[0;32m    301\u001b[0m     vitdet,\n\u001b[0;32m    302\u001b[0m     vitmatte,\n\u001b[0;32m    303\u001b[0m     vitpose,\n\u001b[0;32m    304\u001b[0m     vitpose_backbone,\n\u001b[0;32m    305\u001b[0m     vits,\n\u001b[0;32m    306\u001b[0m     vivit,\n\u001b[0;32m    307\u001b[0m     wav2vec2,\n\u001b[0;32m    308\u001b[0m     wav2vec2_bert,\n\u001b[0;32m    309\u001b[0m     wav2vec2_conformer,\n\u001b[0;32m    310\u001b[0m     wav2vec2_phoneme,\n\u001b[0;32m    311\u001b[0m     wav2vec2_with_lm,\n\u001b[0;32m    312\u001b[0m     wavlm,\n\u001b[0;32m    313\u001b[0m     whisper,\n\u001b[0;32m    314\u001b[0m     x_clip,\n\u001b[0;32m    315\u001b[0m     xglm,\n\u001b[0;32m    316\u001b[0m     xlm,\n\u001b[0;32m    317\u001b[0m     xlm_roberta,\n\u001b[0;32m    318\u001b[0m     xlm_roberta_xl,\n\u001b[0;32m    319\u001b[0m     xlnet,\n\u001b[0;32m    320\u001b[0m     xmod,\n\u001b[0;32m    321\u001b[0m     yolos,\n\u001b[0;32m    322\u001b[0m     yoso,\n\u001b[0;32m    323\u001b[0m     zamba,\n\u001b[0;32m    324\u001b[0m     zamba2,\n\u001b[0;32m    325\u001b[0m     zoedepth,\n\u001b[0;32m    326\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1040\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for testing only\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, embedder, collection, retrieve_number=3, gpu_based=False, history_length=3):\n",
    "        model_id = \"stabilityai/stablelm-2-zephyr-1_6b\" if gpu_based else \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        self.device = \"cuda\" if self.gpu_based else \"cpu\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_id, device_map=self.device)\n",
    "        self.embedder = embedder\n",
    "        self.retriever = collection\n",
    "        self.retrieve_number = retrieve_number\n",
    "        self.gpu_based = gpu_based\n",
    "        self.history = []\n",
    "        self.history_length = history_length\n",
    "\n",
    "    def generate(self, query):\n",
    "        query_embedding = self.embedder.encode(query.lower()).tolist()\n",
    "        results = self.retriever.query(query_embeddings=[query_embedding], n_results=self.retrieve_number)\n",
    "        prompt = self.build_prompt(query, results)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        output = output.split(\"Correct answer:\")[1].strip()\n",
    "        self.history.append((query, output))\n",
    "        self.history = self.history[-self.history_length:]\n",
    "        return output\n",
    "\n",
    "    def context_from_results(self, results):\n",
    "        contexts = []\n",
    "        for document, metadata in zip(results[\"documents\"], results[\"metadatas\"]):\n",
    "            metadata = metadata[0]\n",
    "            document = document[0]\n",
    "            if metadata[\"question\"]:\n",
    "                contexts.append(metadata[\"answer\"])\n",
    "            else:\n",
    "                contexts.append(document)\n",
    "        return contexts\n",
    "\n",
    "    def build_prompt(self, query, results):\n",
    "        contexts = self.context_from_results(results)\n",
    "        history_str = \"\"\n",
    "        if self.history:\n",
    "            history_str = \"\\n\".join([f\"User: {q}\\nAssistant: {a}\" for q, a in self.history]) + \"\\n\"\n",
    "\n",
    "        return f'''\n",
    "            You are a helpful coding assistant specializing in Python and software engineering.\n",
    "            Always include a relevant code example if appropriate.\n",
    "            If you do not know the answer, say \"I don't know.\" Do not fabricate information.\n",
    "            If needed, ask the user for clarification.\n",
    "            Previous conversation:\n",
    "            {history_str}\n",
    "            Use the following context snippets (if relevant) to answer the user's current question.\n",
    "            Context snippets:\n",
    "            \\\"\\\"\\\"\\n{contexts}\\\"\\\"\\\"\n",
    "\n",
    "            Current question: {query}\n",
    "\n",
    "            Correct answer:'''.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d58d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLLM:\n",
    "    def __init__(self, gpu_based=False):\n",
    "        model_id = \"stabilityai/stablelm-2-zephyr-1_6b\" if gpu_based else \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "        self.device = \"cuda\" if self.gpu_based else \"cpu\"\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(model_id, device_map=self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        self.gpu_based = gpu_based\n",
    "\n",
    "    def generate(self, query):\n",
    "        inputs = self.tokenizer(query, return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.llm.generate(**inputs, max_new_tokens=200)\n",
    "        output = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return output.split(\"?\")[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c6c037",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RAG' object has no attribute 'gpu_based'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# tuki nej bo vprašaj na koncu (nej bo samo en vprašaj)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow to parse json in python?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m code_llm \u001b[38;5;241m=\u001b[39m \u001b[43mRAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m answer \u001b[38;5;241m=\u001b[39m code_llm\u001b[38;5;241m.\u001b[39mgenerate(query)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAG answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m, in \u001b[0;36mRAG.__init__\u001b[1;34m(self, embedder, collection, retrieve_number, gpu_based)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, embedder, collection, retrieve_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, gpu_based\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m      6\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stablelm-2-zephyr-1_6b\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gpu_based \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpu_based\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RAG' object has no attribute 'gpu_based'"
     ]
    }
   ],
   "source": [
    "# tuki nej bo vprašaj na koncu (nej bo samo en vprašaj)\n",
    "query = \"How to parse json in python?\"\n",
    "\n",
    "code_llm = RAG(model, collection)\n",
    "answer = code_llm.generate(query)\n",
    "print(f\"RAG answer:\\n{answer}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "basicLLM = BasicLLM()\n",
    "basic_answer = basicLLM.generate(query)\n",
    "print(f\"Basic answer:\\n{basic_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bc1433",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "- prepare questions\n",
    "- get answers\n",
    "- evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a651cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_questions = [\"How does Python's if __name__ == '__main__': work?\",\n",
    "\"Explain the use of *args and **kwargs in function definitions.\",\n",
    "\"What is the difference between .loc[] and .iloc[] in Pandas?\",\n",
    "\"How do you handle missing values in a DataFrame?\",\n",
    "\"How do you write unit tests in Python using unittest?\",\n",
    "\"What is the purpose of __init__.py in a Python package?\",\n",
    "\"How does inheritance work in Python classes?\",\n",
    "\"What is the purpose of __str__ and __repr__ methods?\",\n",
    "\"What is a virtual environment and how do you use it?\",\n",
    "\"What is NumPy and how is it used?\",\n",
    "\"How can I set a fixed sized font for text in my figure using matplotlib?\",\n",
    "\"How do you train a neural network using pyTorch?\",\n",
    "\"How do you merge two dataframes in Pandas?\",\n",
    "\"How do you save a sci-kit learn model?\"]\n",
    "\n",
    "simple_answers = [\n",
    "    \"The statement `if __name__ == '__main__':` checks whether the script is being run directly (not imported as a module). If so, the code under this block will execute.\",\n",
    "    \"`*args` allows a function to accept any number of positional arguments, and `**kwargs` allows it to accept any number of keyword arguments.\",\n",
    "    \"In Pandas, `.loc[]` is label-based indexing (using row and column names), while `.iloc[]` is integer position-based indexing.\",\n",
    "    \"Missing values in a DataFrame can be handled using methods like `df.fillna()` to fill them or `df.dropna()` to remove rows or columns with missing data.\",\n",
    "    \"To write unit tests with `unittest`, define a class that inherits from `unittest.TestCase` and write methods starting with `test_`. Then run tests using `unittest.main()`.\",\n",
    "    \"`__init__.py` marks a directory as a Python package and can also be used to execute package initialization code.\",\n",
    "    \"Inheritance allows a class (child) to inherit methods and properties from another class (parent) using `class Child(Parent):` syntax.\",\n",
    "    \"`__str__` defines the human-readable string representation of an object, while `__repr__` defines the unambiguous representation used for debugging.\",\n",
    "    \"A virtual environment is an isolated Python environment. You create it using `python -m venv env`, activate it, and install dependencies inside it.\",\n",
    "    \"NumPy is a library for numerical computing in Python, offering fast operations on arrays and matrices, and tools for linear algebra, statistics, and more.\",\n",
    "    \"In matplotlib, you can set a fixed-size font using: `plt.rcParams['font.size'] = 12` or by specifying `fontsize=12` in individual plot elements.\",\n",
    "    \"In PyTorch, you define a model class, a loss function, and an optimizer, then train in a loop by doing forward pass, computing loss, backpropagation (`loss.backward()`), and `optimizer.step()`.\",\n",
    "    \"Use `pd.merge(df1, df2, on='key')` to merge two dataframes on a common column, similar to SQL joins.\",\n",
    "    \"You can save a scikit-learn model using `import joblib` and `joblib.dump(model, 'model.pkl')`. Load it later with `joblib.load('model.pkl')`.\"\n",
    "]\n",
    "\n",
    "\n",
    "semi_adv_instructions = [\"Write code for removing all duplicate elements of a list.\",\n",
    "\"Reverse a list in Python without using built-in methods?\",\n",
    "\"Write a function to check if a string is a palindrome.\",\n",
    "\"Check for duplicates in a specific column in pandas Dataframe\",\n",
    "\"Make an HTTP GET request in Python?\",\n",
    "\"Serialize a Python object using pickle\",\n",
    "\"Train a linear regression model with Scikit-Learn\",\n",
    "\"Find the maximum value in a python list\",\n",
    "\"Write a Python code to check if a number is even or odd.\",\n",
    "\"Write a Python program to find the intersection of two sets.\",\n",
    "\"Implement a simple calculator in Python\",\n",
    "\"Write a Python code to convert a list of temperatures from Celsius to Fahrenheit.\",\n",
    "\"Save and load a NumPy array to and from a file\"]\n",
    "\n",
    "semi_adv_answers = [\n",
    "    \"You can remove duplicates using: `unique_list = list(set(original_list))`.\",\n",
    "    \"You can reverse a list manually like this:\\n```python\\nreversed_list = [original_list[i] for i in range(len(original_list)-1, -1, -1)]\\n```\",\n",
    "    \"Here's a palindrome checker:\\n```python\\ndef is_palindrome(s):\\n    return s == s[::-1]\\n```\",\n",
    "    \"Check duplicates in a column using:\\n```python\\nduplicates = df[df['column_name'].duplicated()]\\n```\",\n",
    "    \"Use the `requests` library:\\n```python\\nimport requests\\nresponse = requests.get('https://example.com')\\nprint(response.text)\\n```\",\n",
    "    \"Pickle an object with:\\n```python\\nimport pickle\\nwith open('obj.pkl', 'wb') as f:\\n    pickle.dump(obj, f)\\n```\",\n",
    "    \"Train linear regression:\\n```python\\nfrom sklearn.linear_model import LinearRegression\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n```\",\n",
    "    \"Find the max value with:\\n```python\\nmaximum = max(my_list)\\n```\",\n",
    "    \"Even/odd check:\\n```python\\ndef check_even_odd(n):\\n    return 'Even' if n % 2 == 0 else 'Odd'\\n```\",\n",
    "    \"Find intersection:\\n```python\\nintersection = set1 & set2\\n```\",\n",
    "    \"Simple calculator:\\n```python\\ndef calc(a, b, op):\\n    if op == '+': return a + b\\n    elif op == '-': return a - b\\n    elif op == '*': return a * b\\n    elif op == '/': return a / b\\n```\",\n",
    "    \"Convert Celsius to Fahrenheit:\\n```python\\nfahrenheit = [c * 9/5 + 32 for c in celsius_list]\\n```\",    \n",
    "    \"Save/load NumPy array:\\n```python\\nimport numpy as np\\nnp.save('array.npy', arr)\\nloaded = np.load('array.npy')\\n```\"\n",
    "]\n",
    "\n",
    "advanced_instructions = [\"Plan and provide code for a weather app. Provide an implemetation plan, design, code, instructions for hosting and anything else that might be needed.\",\n",
    "\"Write complete code for RAG pipeline. Include unit tests for each step.\",\n",
    "\"Can you explain backpropagation and provide a concrete teaching example?\",\n",
    "\"How does the Cholesky decomposition work? Provide python code.\",\n",
    "\"How would you explain RSA to a child?\",\n",
    "\"Create me a project for my subject \"algorithms and data structures\". Make it worth 3ECTS.\",\n",
    "\"Using python, how can I fill my VIRT memory on a Linux machine? What about SHR?\"]\n",
    "\n",
    "advanced_answers = [\n",
    "    \"To build a weather app:\\n1. **Plan**: Fetch real-time weather data using an API like OpenWeatherMap.\\n2. **Design**: UI with city input, forecast display, and icons for weather types.\\n3. **Code**: Use Python (Flask or FastAPI) for backend, and HTML/CSS/JavaScript or Streamlit for frontend.\\n4. **Instructions**: Host on Heroku or Render. Set up API keys securely. Use requests for API calls.\\n5. **Extra**: Add caching, error handling, unit tests, and optional location auto-detection.\",\n",
    "    \"A basic RAG pipeline includes: document chunking, embedding generation (e.g., with SentenceTransformers), vector store retrieval (e.g., FAISS), and LLM integration.\\nUnit tests should cover:\\n- Chunking edge cases\\n- Embedding shape and type checks\\n- Retrieval relevance accuracy\\n- Response generation fidelity using mocked LLM output.\",\n",
    "    \"Backpropagation is how neural networks learn: it computes gradients of the loss with respect to weights by applying the chain rule layer-by-layer backward.\\nExample: For a 2-layer MLP, manually compute derivatives of the loss (e.g., MSE) with respect to weights and biases using a small input/output example.\",\n",
    "    \"Cholesky decomposition factorizes a symmetric positive-definite matrix `A` into `L * L.T`, where `L` is lower triangular.\\nExample in Python:\\n```python\\nimport numpy as np\\nA = np.array([[4, 2], [2, 3]])\\nL = np.linalg.cholesky(A)\\n```\",\n",
    "    \"RSA explained for a child:\\nImagine a locked mailbox. Anyone can drop a letter in (encrypt with your public key), but only you have the key to open it (your private key).\",\n",
    "    \"A 3 ECTS project idea:\\n**Title**: 'Visual Algorithm Simulator'\\n**Scope**: Implement and visualize sorting/searching algorithms, graph traversal, and data structures using Python + Tkinter or web (e.g., React + Flask).\\n**Deliverables**: Code, report, performance benchmarks, usage instructions.\\n**Learning Outcomes**: Algorithm analysis, data structure design, visualization, testing.\",    \n",
    "    \"In Python on Linux:\\n- **To fill VIRT memory** (virtual address space): allocate large unused arrays, e.g., `big = bytearray(10**9)`.\\n- **To fill SHR (shared memory)**: spawn multiprocessing workers or load shared libraries heavily reused across processes.\"\n",
    "]\n",
    "\n",
    "debugging_questiions = [\n",
    "\"\"\"fix my code:\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch_data():\n",
    "    response = aiohttp.ClientSession().get('https://example.com')\n",
    "    data = await response.text()\n",
    "    print(data)\n",
    "\n",
    "asyncio.run(fetch_data())\"\"\",\n",
    "\"\"\"\n",
    "fix my code:\n",
    "df = pd.DataFrame({'A': [1, 2, 3]})\n",
    "df[df['A'] > 1]['A'] = 0\n",
    "print(df)\"\"\",\n",
    "\"\"\"fix my code:\n",
    "def append_item(item, lst=[]):\n",
    "    lst.append(item)\n",
    "    return lst\n",
    "\n",
    "print(append_item(1))\n",
    "print(append_item(2))\n",
    "\"\"\",\n",
    "\"\"\"fix my code:\n",
    "def outer():\n",
    "    x = 10\n",
    "    def inner():\n",
    "        print(x)\n",
    "        x += 1\n",
    "    inner()\n",
    "\n",
    "outer()\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "fix my error: RuntimeError: This event loop is already running\n",
    "for code:\n",
    "import asyncio\n",
    "\n",
    "async def say_hello():\n",
    "    await asyncio.sleep(1)\n",
    "    return \"Hello\"\n",
    "\n",
    "def main():\n",
    "    loop = asyncio.get_event_loop()\n",
    "    result = loop.run_until_complete(say_hello())\n",
    "    print(result)\n",
    "\n",
    "main()\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "fix my error: AttributeError: 'Person' object has no attribute 'gender'\n",
    "for code:\n",
    "class Person:\n",
    "    __slots__ = ['name', 'age']\n",
    "\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p = Person(\"Alice\", 30)\n",
    "p.gender = \"female\"\n",
    "\"\"\"]\n",
    "\n",
    "debugging_answers = [\n",
    "    # 1. Async/Aiohttp fix\n",
    "    \"\"\"import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch_data():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.get('https://example.com') as response:\n",
    "            data = await response.text()\n",
    "            print(data)\n",
    "\n",
    "asyncio.run(fetch_data())\"\"\",\n",
    "\n",
    "    # 2. Pandas chained assignment fix\n",
    "    \"\"\"import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'A': [1, 2, 3]})\n",
    "df.loc[df['A'] > 1, 'A'] = 0\n",
    "print(df)\"\"\",\n",
    "\n",
    "    # 3. Mutable default argument fix\n",
    "    \"\"\"def append_item(item, lst=None):\n",
    "    if lst is None:\n",
    "        lst = []\n",
    "    lst.append(item)\n",
    "    return lst\n",
    "\n",
    "print(append_item(1))\n",
    "print(append_item(2))\"\"\",\n",
    "\n",
    "    # 4. UnboundLocalError in closure fix\n",
    "    \"\"\"def outer():\n",
    "    x = 10\n",
    "    def inner():\n",
    "        nonlocal x\n",
    "        print(x)\n",
    "        x += 1\n",
    "    inner()\n",
    "\n",
    "outer()\"\"\",\n",
    "\n",
    "    # 5. Event loop already running fix (for notebook environments)\n",
    "    \"\"\"import asyncio\n",
    "\n",
    "async def say_hello():\n",
    "    await asyncio.sleep(1)\n",
    "    return \"Hello\"\n",
    "\n",
    "# For environments like Jupyter:\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def main():\n",
    "    result = await say_hello()\n",
    "    print(result)\n",
    "\n",
    "asyncio.run(main())\"\"\",\n",
    "\n",
    "    # 6. __slots__ attribute error fix\n",
    "    \"\"\"class Person:\n",
    "    __slots__ = ['name', 'age', 'gender']\n",
    "\n",
    "    def __init__(self, name, age):\n",
    "        self.name = name\n",
    "        self.age = age\n",
    "\n",
    "p = Person(\"Alice\", 30)\n",
    "p.gender = \"female\"\n",
    "\"\"\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7a091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_responses = [code_llm.generate(query) for query in simple_questions]\n",
    "for ans, gt in zip(simple_responses, simple_answers):\n",
    "    print(\"RAG output:\")\n",
    "    print(ans)\n",
    "    print(\"---------------------\")\n",
    "    print(\"ChatGPT (GPT-4 turbo) output:\")\n",
    "    print(gt)\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "semi_adv_responses = [code_llm.generate(query) for query in semi_adv_instructions]\n",
    "for ans, gt in zip(semi_adv_responses, semi_adv_answers):\n",
    "    print(\"RAG output:\")\n",
    "    print(ans)\n",
    "    print(\"---------------------\")\n",
    "    print(\"ChatGPT (GPT-4 turbo) output:\")\n",
    "    print(gt)\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "advanced_responses = [code_llm.generate(query) for query in advanced_instructions]\n",
    "for ans, gt in zip(advanced_responses, advanced_answers):\n",
    "    print(\"RAG output:\")\n",
    "    print(ans)\n",
    "    print(\"---------------------\")\n",
    "    print(\"ChatGPT (GPT-4 turbo) output:\")\n",
    "    print(gt)\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")\n",
    "    print(\"----------------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
